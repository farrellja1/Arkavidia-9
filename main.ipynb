{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path):\n",
    "    dataframes = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            category = os.path.splitext(filename)[0]\n",
    "            df['Category'] = category\n",
    "            dataframes.append(df)\n",
    "    if dataframes:\n",
    "        merged_data = pd.concat(dataframes, ignore_index=True)\n",
    "        merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "    else:\n",
    "        print(\"No CSV files found in the specified folder.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data2(base_folder_path):\n",
    "    dataframes = []\n",
    "    \n",
    "    for category in os.listdir(base_folder_path):\n",
    "        category_path = os.path.join(base_folder_path, category)\n",
    "        \n",
    "        if os.path.isdir(category_path):\n",
    "            for filename in os.listdir(category_path):\n",
    "                if filename.endswith('.csv'):\n",
    "                    file_path = os.path.join(category_path, filename)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    df['Category'] = category\n",
    "                    province = os.path.splitext(filename)[0]\n",
    "                    df['Province'] = province\n",
    "                    price_columns = df.columns.difference(['Date', 'Category', 'Province'])\n",
    "                    if not price_columns.empty:\n",
    "                        df['Price'] = df[price_columns[0]]\n",
    "                    else:\n",
    "                        print(f\"No price column found in {filename}. Skipping this file.\")\n",
    "                        continue\n",
    "                    \n",
    "                    dataframes.append(df)\n",
    "    \n",
    "    if dataframes:\n",
    "        merged_data = pd.concat(dataframes, ignore_index=True)\n",
    "        merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "    else:\n",
    "        print(\"No CSV files found in the specified folder.\")\n",
    "        return pd.DataFrame()\n",
    "    merged_data2 = merged_data.pivot_table(index=['Date', 'Category'], columns='Province', values='Price', aggfunc='first')\n",
    "\n",
    "    merged_data2.reset_index(inplace=True)\n",
    "    merged_data2['Category'] = merged_data2['Category'].replace({\n",
    "    'bawang merah': 'Bawang Merah',\n",
    "    'bawang putih': 'Bawang Putih Bonggol',\n",
    "    'cabai merah': 'Cabai Merah Keriting',\n",
    "    'cabai rawit': 'Cabai Rawit Merah',\n",
    "    'daging ayam': 'Daging Ayam Ras',\n",
    "    'daging sapi': 'Daging Sapi Murni',\n",
    "    'gula': 'Gula Konsumsi',\n",
    "    'telur ayam': 'Telur Ayam Ras',\n",
    "    'tepung terigu': 'Tepung Terigu (Curah)',\n",
    "})\n",
    "    return merged_data2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_trend(folder_path):\n",
    "    dataframes = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df['Category'] = filename[:-4]\n",
    "            df.rename(columns={'Riau Islands': 'Kepulauan Riau', 'Bangka Belitung Islands' : 'Kepulauan Bangka Belitung',\n",
    "                               'Sumatra Barat' : 'Sumatera Barat', 'Sumatra Utara' : 'Sumatera Utara',\n",
    "                               'Sumatra Selatan' : 'Sumatera Selatan'}, inplace=True)\n",
    "            dataframes.append(df)\n",
    "\n",
    "    if dataframes:\n",
    "        merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "        merged_df['Category'] = merged_df['Category'].replace({\n",
    "    'bawang merah': 'Bawang Merah',\n",
    "    'bawang putih': 'Bawang Putih Bonggol',\n",
    "    'cabai merah': 'Cabai Merah Keriting',\n",
    "    'cabai rawit': 'Cabai Rawit Merah',\n",
    "    'daging ayam': 'Daging Ayam Ras',\n",
    "    'daging sapi': 'Daging Sapi Murni',\n",
    "    'gula': 'Gula Konsumsi',\n",
    "    'telur ayam': 'Telur Ayam Ras',\n",
    "    'tepung terigu': 'Tepung Terigu (Curah)',\n",
    "})\n",
    "        return merged_df\n",
    "    else:\n",
    "        print(\"No CSV files found in the specified folder.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna(df):\n",
    "    missing_percentage = df.isnull().sum() / len(df) * 100\n",
    "    for column in df.columns:\n",
    "        # Check if the percentage of missing values is less than 10%\n",
    "        if missing_percentage[column] < 10:\n",
    "            # Forward fill for less than 10% missing values\n",
    "            # df[column] = df[column].fillna(method='ffill')\n",
    "            df[column] = df[column].fillna(0)\n",
    "        elif missing_percentage[column] < 50:\n",
    "            # # Check for NaNs in the first rows\n",
    "            # if df[column].isnull().any():\n",
    "            #     # Forward fill initial NaNs\n",
    "            #     df[column] = df[column].fillna(method='ffill')\n",
    "            \n",
    "            # # Now apply Exponential Smoothing\n",
    "            # # Ensure there are enough non-null values to fit the model\n",
    "            # if df[column].notnull().sum() >= 10:  # Check for at least 10 non-null values\n",
    "            #     model = ExponentialSmoothing(df[column], trend='add', seasonal='add', seasonal_periods=7, initialization_method='estimated')\n",
    "            #     model_fit = model.fit()\n",
    "            #     df[column] = model_fit.fittedvalues\n",
    "            # else:\n",
    "            #     # If not enough data, you can choose to fill with mean or drop\n",
    "            #     mean_value = df[column].mean()\n",
    "            #     df[column] = df[column].fillna(mean_value)\n",
    "            # df[column] = df[column].fillna(method='ffill')\n",
    "            df[column] = df[column].fillna(0)\n",
    "            # df[column] = df[column].interpolate(method='akima')\n",
    "        # else:\n",
    "        #     df[column] = df[column].fillna(method='ffill')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt(df, value_name):\n",
    "    melted = df.melt(id_vars=['Date', 'Category'], var_name='Province', value_name=value_name)\n",
    "    return melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge(df1, df2):\n",
    "    merged_data = pd.merge(df1, df2, on=['Date', 'Category', 'Province'], how='left')\n",
    "    quantities = df2[df2['Category'].isin(['beras', 'minyak goreng'])].groupby(['Date', 'Province', 'Category'])['Quantity'].sum().unstack(fill_value=0)\n",
    "    quantities.columns = ['beras', 'minyak_goreng']\n",
    "    merged_data = merged_data.merge(quantities, on=['Date', 'Province'], how='left')\n",
    "    merged_data.loc[merged_data['Category'] == 'Beras Medium', 'Quantity'] = merged_data['beras']\n",
    "    merged_data.loc[merged_data['Category'] == 'Beras Premium', 'Quantity'] = merged_data['beras']\n",
    "    merged_data.loc[merged_data['Category'] == 'Minyak Goreng Curah', 'Quantity'] = merged_data['minyak_goreng']\n",
    "    merged_data.loc[merged_data['Category'] == 'Minyak Goreng Kemasan Sederhana', 'Quantity'] = merged_data['minyak_goreng']\n",
    "    merged_data.drop(columns=['beras', 'minyak_goreng'], inplace=True)\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, column):\n",
    "    \"\"\"\n",
    "    One-hot encodes a categorical feature in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the categorical feature.\n",
    "    column (str): The name of the column to be one-hot encoded.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with the one-hot encoded features.\n",
    "    \"\"\"\n",
    "    # Perform one-hot encoding\n",
    "    one_hot_encoded_df = pd.get_dummies(df, columns=[column], drop_first=True)\n",
    "    return one_hot_encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(folder_path, base_folder_path, isTrain):\n",
    "    data = load_data(folder_path)\n",
    "    if isTrain:\n",
    "        data2 = load_data2(base_folder_path)\n",
    "        data2 = fillna(data2)\n",
    "    else:\n",
    "        data2 = google_trend(base_folder_path)\n",
    "    data_melted = melt(data, 'Price')\n",
    "    data2_melted = melt(data2, 'Quantity')\n",
    "    data_merged = merge(data_melted, data2_melted)\n",
    "    # data_merged = one_hot_encode(data_merged, 'Province')\n",
    "    return data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farel\\AppData\\Local\\Temp\\ipykernel_34112\\2598489094.py:25: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_data = pd.concat(dataframes, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'C:\\Users\\farel\\OneDrive\\Documents\\GitHub\\Arkavidia-9\\Harga Bahan Pangan\\train'\n",
    "folder_path2 = r'C:\\Users\\farel\\OneDrive\\Documents\\GitHub\\Arkavidia-9\\Google Trend'\n",
    "train = pipeline(folder_path, folder_path2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "train['Date'] =train['Date'].astype('int64') // 10**9\n",
    "train['Category'] = train['Category'].astype('category')\n",
    "train['Province'] = train['Province'].astype('category')\n",
    "train = train.dropna(subset=['Price'])\n",
    "X = train.drop(columns=['Price', 'Quantity'])\n",
    "y = train['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): True values.\n",
    "    y_pred (array-like): Predicted values.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated MAPE.\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10, enable_categorical=True)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.451369130362146"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 559\n",
      "[LightGBM] [Info] Number of data points in the train set: 338071, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 36140.583753\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "model = LGBMRegressor(random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2722734405020555"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.266117540446549"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hist gradient boosting\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "\n",
    "\n",
    "model = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mape(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 559\n",
      "[LightGBM] [Info] Number of data points in the train set: 338071, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 36140.583753\n",
      "MAPE: 3.53%\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import KFold  # Change this line\n",
    "\n",
    "# Define your model\n",
    "model = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Define the parameter space for Bayesian optimization\n",
    "param_space = {\n",
    "    'n_estimators': (10, 100),\n",
    "    'max_depth': (1, 10),\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'num_leaves': (2, 50),\n",
    "    'min_child_samples': (1, 20),\n",
    "    'subsample': (0.05, 1.0),\n",
    "    'colsample_bytree': (0.1, 1.0),\n",
    "}\n",
    "\n",
    "# Use KFold for regression tasks\n",
    "opt = BayesSearchCV(model, param_space, n_iter=32, cv=KFold(n_splits=5), n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = opt.predict(X_test)\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE: {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: OrderedDict([('colsample_bytree', 1.0), ('learning_rate', 0.4319256222176345), ('max_depth', 10), ('min_child_samples', 20), ('n_estimators', 70), ('num_leaves', 50), ('subsample', 0.21727297425491027)])\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 559\n",
      "[LightGBM] [Info] Number of data points in the train set: 338071, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 36140.583753\n",
      "MAPE with Best Model: 3.53%\n"
     ]
    }
   ],
   "source": [
    "best_params = opt.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Optionally, create a new model with the best parameters\n",
    "best_model = LGBMRegressor(**best_params, random_state=42)\n",
    "\n",
    "# Fit the best model on the training data (if needed)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_best_model = best_model.predict(X_test)\n",
    "\n",
    "# Calculate MAPE for the best model\n",
    "mape_best_model = np.mean(np.abs((y_test - y_pred_best_model) / y_test)) * 100\n",
    "print(f'MAPE with Best Model: {mape_best_model:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 2.12%\n",
      "Best Parameters: OrderedDict([('l2_regularization', 10.0), ('learning_rate', 0.420324924561727), ('max_depth', 7), ('max_iter', 1000), ('min_samples_leaf', 20)])\n",
      "MAPE with Best Model: 2.12%\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "model = HistGradientBoostingRegressor(random_state=42)\n",
    "param_space = {\n",
    "    'max_iter': (100, 1000),\n",
    "    'max_depth': (1, 10),\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'min_samples_leaf': (1, 20),\n",
    "    'l2_regularization': (0.0, 10.0)\n",
    "}\n",
    "opt = BayesSearchCV(model, param_space, n_iter=32, cv=KFold(n_splits=5), n_jobs=-1, random_state=42)\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = opt.predict(X_test)\n",
    "\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE: {mape:.2f}%')\n",
    "\n",
    "best_params = opt.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "best_model = HistGradientBoostingRegressor(**best_params, random_state=42)\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred_best_model = best_model.predict(X_test)\n",
    "\n",
    "mape_best_model = np.mean(np.abs((y_test - y_pred_best_model) / y_test)) * 100\n",
    "print(f'MAPE with Best Model: {mape_best_model:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 2.099736331028919%\n"
     ]
    }
   ],
   "source": [
    "model = HistGradientBoostingRegressor(l2_regularization=10.0, learning_rate=0.420324924561727,\n",
    "                                      max_depth=7, max_iter=1000, min_samples_leaf=20, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE: {mape:}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: 1.9571\n",
      "Province: 0.0919\n",
      "Date: 0.0673\n"
     ]
    }
   ],
   "source": [
    "#feature importances\n",
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=42)\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "importances = result.importances_mean\n",
    "feature_names = X_train.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Display feature importances\n",
    "for i in range(len(importances)):\n",
    "    print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 2.12%\n"
     ]
    }
   ],
   "source": [
    "y_pred = opt.predict(X_test)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAPE: {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_test = r'C:\\Users\\farel\\OneDrive\\Documents\\GitHub\\Arkavidia-9\\Harga Bahan Pangan\\test'\n",
    "folder_path2 = r'C:\\Users\\farel\\OneDrive\\Documents\\GitHub\\Arkavidia-9\\google_trend'\n",
    "test = pipeline(folder_test, folder_path2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Date'] = pd.to_datetime(test['Date'])\n",
    "test['Date'] =test['Date'].astype('int64') // 10**9\n",
    "test['Category'] = test['Category'].astype('category')\n",
    "test['Province'] = test['Province'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = test.drop(columns=['Price'])\n",
    "test['Price'] = model.predict(testing)\n",
    "test['Date'] = pd.to_datetime(test['Date'], unit='s')\n",
    "test['id'] = test['Category'].astype(str) + '/' + test['Province'].astype(str) + '/' + test['Date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = test[['id', 'Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(r'C:\\Users\\farel\\OneDrive\\Documents\\GitHub\\Arkavidia-9\\Harga Bahan Pangan\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = prediction.set_index('id').reindex(sample_submission['id']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bawang Merah/Aceh/2024-10-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bawang Merah/Aceh/2024-10-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bawang Merah/Aceh/2024-10-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bawang Merah/Aceh/2024-10-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bawang Merah/Aceh/2024-10-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40659</th>\n",
       "      <td>Tepung Terigu (Curah)/Sumatera Utara/2024-12-27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40660</th>\n",
       "      <td>Tepung Terigu (Curah)/Sumatera Utara/2024-12-28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40661</th>\n",
       "      <td>Tepung Terigu (Curah)/Sumatera Utara/2024-12-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40662</th>\n",
       "      <td>Tepung Terigu (Curah)/Sumatera Utara/2024-12-30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40663</th>\n",
       "      <td>Tepung Terigu (Curah)/Sumatera Utara/2024-12-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40664 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    id  price\n",
       "0                         Bawang Merah/Aceh/2024-10-01      0\n",
       "1                         Bawang Merah/Aceh/2024-10-02      0\n",
       "2                         Bawang Merah/Aceh/2024-10-03      0\n",
       "3                         Bawang Merah/Aceh/2024-10-04      0\n",
       "4                         Bawang Merah/Aceh/2024-10-05      0\n",
       "...                                                ...    ...\n",
       "40659  Tepung Terigu (Curah)/Sumatera Utara/2024-12-27      0\n",
       "40660  Tepung Terigu (Curah)/Sumatera Utara/2024-12-28      0\n",
       "40661  Tepung Terigu (Curah)/Sumatera Utara/2024-12-29      0\n",
       "40662  Tepung Terigu (Curah)/Sumatera Utara/2024-12-30      0\n",
       "40663  Tepung Terigu (Curah)/Sumatera Utara/2024-12-31      0\n",
       "\n",
       "[40664 rows x 2 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
